{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLbr3g_jYEIu"
      },
      "source": [
        "1. Submit the Python script containing all your implementations for tokenization, stopword removal, and named entity extraction.\n",
        "\n",
        "2. Provide answers to the following questions:\n",
        "\n",
        "  - What are the advantages and limitations of NLTK and spaCy in text prepocessing?\n",
        "\n",
        "  - How can tokenization help with analyzing customer feedback?\n",
        "\n",
        "  - How does removing stop words impact the analysis.\n",
        "\n",
        "  - Why is it important to extract named entities from customer feedback?\n",
        "\n",
        "  - What insights would you look for from tokenized feedback?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install required Libraries"
      ],
      "metadata": {
        "id": "PKaKjEgFb27s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wpB8-u4jX8I3",
        "outputId": "e09fbd2e-f79a-4c3e-af74-1a046e3b887f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.13.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EDq5XvJPY5iN",
        "outputId": "a14f5636-5254-486e-94d2-fe77565fbdab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "spacy.cli.download(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FajK9i1YbyXy",
        "outputId": "499fc814-9ff7-4a0a-f422-4bec0e21d488"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK Method"
      ],
      "metadata": {
        "id": "mENlemhkd1el"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing"
      ],
      "metadata": {
        "id": "52CXK621eVQT"
      }
    },
    {
      "source": [
        "import nltk\n",
        "\n",
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Great product, but the software crashed twice in the last week. The customer support team was very helpful, though. Could improve the battery life.\"\n",
        "tokens = word_tokenize(text)\n",
        "tokens"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qal6dL4PdWAJ",
        "outputId": "affc9a66-3e11-44c9-95fc-898c48d73932"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Great',\n",
              " 'product',\n",
              " ',',\n",
              " 'but',\n",
              " 'the',\n",
              " 'software',\n",
              " 'crashed',\n",
              " 'twice',\n",
              " 'in',\n",
              " 'the',\n",
              " 'last',\n",
              " 'week',\n",
              " '.',\n",
              " 'The',\n",
              " 'customer',\n",
              " 'support',\n",
              " 'team',\n",
              " 'was',\n",
              " 'very',\n",
              " 'helpful',\n",
              " ',',\n",
              " 'though',\n",
              " '.',\n",
              " 'Could',\n",
              " 'improve',\n",
              " 'the',\n",
              " 'battery',\n",
              " 'life',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing stopwords"
      ],
      "metadata": {
        "id": "qPEfYsQaeXkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens_nltk = [words for words in tokens if not words.lower() in stop_words]\n",
        "filtered_tokens_nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iLWD3beKd5_2",
        "outputId": "68c7e467-904a-4f49-cedb-a809c9141262"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Great',\n",
              " 'product',\n",
              " ',',\n",
              " 'software',\n",
              " 'crashed',\n",
              " 'twice',\n",
              " 'last',\n",
              " 'week',\n",
              " '.',\n",
              " 'customer',\n",
              " 'support',\n",
              " 'team',\n",
              " 'helpful',\n",
              " ',',\n",
              " 'though',\n",
              " '.',\n",
              " 'Could',\n",
              " 'improve',\n",
              " 'battery',\n",
              " 'life',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spaCY Method\n"
      ],
      "metadata": {
        "id": "vSvST-Lab6og"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing"
      ],
      "metadata": {
        "id": "NqKE8QJZfrpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "doc = nlp(\"Great product, but the software crashed twice in the last week. The customer support team was very helpful, though. Could improve the battery life.\")\n",
        "\n",
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "i4874zo5b_CD",
        "outputId": "37abd22f-0c41-409c-faa5-8415d5e431a5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Great\n",
            "product\n",
            ",\n",
            "but\n",
            "the\n",
            "software\n",
            "crashed\n",
            "twice\n",
            "in\n",
            "the\n",
            "last\n",
            "week\n",
            ".\n",
            "The\n",
            "customer\n",
            "support\n",
            "team\n",
            "was\n",
            "very\n",
            "helpful\n",
            ",\n",
            "though\n",
            ".\n",
            "Could\n",
            "improve\n",
            "the\n",
            "battery\n",
            "life\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing stopwords"
      ],
      "metadata": {
        "id": "yRC70PZufuUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens_spacy = [token.text for token in doc if not token.is_stop]\n",
        "\n",
        "filtered_tokens_spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "AllMEuNMfwCq",
        "outputId": "b48515b3-7ed5-4e20-f4f0-59fbb9ad6fb8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Great',\n",
              " 'product',\n",
              " ',',\n",
              " 'software',\n",
              " 'crashed',\n",
              " 'twice',\n",
              " 'week',\n",
              " '.',\n",
              " 'customer',\n",
              " 'support',\n",
              " 'team',\n",
              " 'helpful',\n",
              " ',',\n",
              " '.',\n",
              " 'improve',\n",
              " 'battery',\n",
              " 'life',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting Named Entities"
      ],
      "metadata": {
        "id": "EPsbacKdfwZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "NFvyrgGegbO9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Analysis\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KksSAGfFjpMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both NLTK and spaCy can effectively tokenize the specified customer feedback, however they differ in their approach and performance. In this scenario, their main difference is the running time and the removing of stopwords. Some stopwords were still included in NLTK while it isn't seen anymore in spaCy, with \"last\" as one example."
      ],
      "metadata": {
        "id": "BuUHUQTDtcAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are the advantages and limitations of NLTK and spaCy in text preprocessing?\n",
        "NLTK is older and more widely used, with broader functionalities, unlike spaCy which merely focuses on core NLP tasks. However, because of this, spaCy runs faster than NLTK at the cost of decreased options for algorithm selections. Other than that, NLTK supports a wide variety of languages whilst spaCy primarily focuses on English, though it has expanded to support other languages but is still limited.\n",
        "\n",
        "With this in mind, deciding between which library to use depends on the user's needs and priorities. NLTK is slower but offers more flexibility and customization, making it more suitable for small-scale projects. Meanwhile, spaCy is more suitable for large-scale projects that require high-speed processing.\n",
        "\n",
        "## How can tokenization help with analyzing customer feedback?\n",
        "Tokenization can help with analyzing customer feedback by improving readability through breaking down the text into smaller units. In doing so, it aids in enabling efficient sentiment nalysis, facilitating better topic modeling, and improving data visualization by grouping together related tokens such as ones that are positive, negative, and neutral.\n",
        "\n",
        "## How does removing stop words impact the analysis.\n",
        "It reduces the noise in NLP algorithms, enhancing the efficiency of the analyses of customer feedbacks. It makes the analysis straight-to-the-point andd targeted. However, depending on the dataset, it may also reduce the accuracy due to how some sentences highly rely on these stopwords for contextual information that the model isn't able to catch onto. As a result, removing stopwords should be carefully considered based on the nature of the data being processed and what the user would like to achieve with the current system.\n",
        "\n",
        "## Why is it important to extract named entities from customer feedback?\n",
        "It is crucial to extract named entities from customer feedback in order for businesses to pinpoint the exact concerns being brought up, enabling more targeted actions--whether by adjusting marketing strategies or knowing which areas for products need improvement. In general, it helps businesses make more data-driven decisions.\n",
        "\n",
        "## What insights would you look for from tokenized feedback?\n",
        "The insights I would look for are common trends when it comes to sentiment analysis. It would reveal patterns in customer opinions, gauging overall customer satisfaction and pinpointing which areas need improvement and other ones to be kept as is."
      ],
      "metadata": {
        "id": "KZhRusMNjxV5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}