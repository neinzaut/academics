{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Tuazon, Francesca Marie A.\n",
        "(BCS34)"
      ],
      "metadata": {
        "id": "F95pYyH_iK5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zik0OBujh8Vr",
        "outputId": "d2f367e6-327d-42a1-c666-68f03c193a94"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# stopwords are common words (is, and, the)\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# download NLTK resources\n",
        "nltk.download('punkt') # punkt is a tokenizer model used for splitting text into sentences (sentence tokenization)\n",
        "nltk.download('stopwords') # breaking down from sentence into words (word tokenization)\n",
        "\n",
        "# load spacy model\n",
        "nlp = English()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHPZVMb8hW36",
        "outputId": "1ff1b5c4-b286-44eb-b49b-936cc7b49e12"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer # CountVectorizer is a tool that converts a collection of text document into matrix of token, it breaks down text into indiv words/tokens and counts how often (freq) each word appears in text\n",
        "from sklearn.naive_bayes import MultinomialNB # MultinomialNB - this is a Naive bayes classifier for multinomially distributed data, often used for text classification - represents freq or counts\n",
        "from sklearn.pipeline import make_pipeline # make_pipeline is a functionused to create a pipeline that sequentially combines several processing steps into single objects\n",
        "\n",
        "# Sample Data\n",
        "texts = [\n",
        "    \"The movie was fantastic, I loved every moment of it\",\n",
        "    \"The food was terrible, I would never eat there again\",\n",
        "    \"I had a great time at the concert\",\n",
        "    \"The service at the restaurant was horrible\",\n",
        "    \"I really enjoyed the book\",\n",
        "    \"The hotel room was dirty and uncomfortable\",\n",
        "    \"I am very satisfied with my purchase\",\n",
        "    \"The delivery was late and the package was damaged\",\n",
        "    \"The customer support was very helpful\",\n",
        "    \"I am disappointed with the quality of the product\"\n",
        "]\n",
        "labels = [\"Positive\", \"Negative\", \"Positive\", \"Negative\", \"Positive\",\n",
        "          \"Negative\", \"Positive\", \"Negative\", \"Positive\", \"Negative\"]\n",
        "\n",
        "# Create a pipeline for the classification\n",
        "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
        "model.fit(texts, labels)\n",
        "\n",
        "# Accept user input\n",
        "user_input = input(\"Enter a sentence for sentiment analysis: \")\n",
        "\n",
        "# Predict the sentiment of the user input\n",
        "prediction = model.predict([user_input])\n",
        "print(\"Predicted sentiment: \", prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyyVCGAokypu",
        "outputId": "5e3eb88e-1006-48ce-a4d9-5852eb48c6fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence for sentiment analysis: the movie was fantastic\n",
            "Predicted sentiment:  ['Positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd # assuming you don't want to import anything\n",
        "\n",
        "\n",
        "# Create CountVectorizer and transform the data\n",
        "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
        "x_count = count_vectorizer.fit_transform(texts)\n",
        "\n",
        "# Split data for CountVectorizer\n",
        "x_train_count, x_test_count, y_train_count, y_test_count = train_test_split(\n",
        "    x_count, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train and predict using Logistic Regression with CountVectorizer features\n",
        "model_count = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model_count.fit(x_train_count, y_train_count)\n",
        "y_pred_count = model_count.predict(x_test_count)\n",
        "\n",
        "# Calculate evaluation metrics for CountVectorizer\n",
        "accuracy_count = accuracy_score(y_test_count, y_pred_count)\n",
        "precision_count = precision_score(y_test_count, y_pred_count, pos_label='Positive')\n",
        "recall_count = recall_score(y_test_count, y_pred_count, pos_label='Positive')\n",
        "f1_count = f1_score(y_test_count, y_pred_count, pos_label='Positive')\n",
        "\n",
        "# Create DataFrame for CountVectorizer evaluation\n",
        "count_eval_df = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
        "    'Score': [accuracy_count, precision_count, recall_count, f1_count]\n",
        "})\n",
        "\n",
        "print(\"CountVectorizer Evaluation:\")\n",
        "print(count_eval_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JU7RkMTSfRyL",
        "outputId": "1514f269-a76f-4c09-a5ab-7f076f49bea7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer Evaluation:\n",
            "      Metric     Score\n",
            "0   Accuracy  0.500000\n",
            "1  Precision  0.500000\n",
            "2     Recall  1.000000\n",
            "3   F1-Score  0.666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd # assuming you don't want to import anything\n",
        "\n",
        "\n",
        "# Create CountVectorizer and transform the data\n",
        "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
        "x_count = count_vectorizer.fit_transform(texts)\n",
        "\n",
        "# Split data for CountVectorizer\n",
        "x_train_count, x_test_count, y_train_count, y_test_count = train_test_split(\n",
        "    x_count, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train and predict using Logistic Regression with CountVectorizer features\n",
        "model_count = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model_count.fit(x_train_count, y_train_count)\n",
        "y_pred_count = model_count.predict(x_test_count)\n",
        "\n",
        "# Calculate evaluation metrics for CountVectorizer\n",
        "accuracy_count = accuracy_score(y_test_count, y_pred_count)\n",
        "precision_count = precision_score(y_test_count, y_pred_count, pos_label='Positive')\n",
        "recall_count = recall_score(y_test_count, y_pred_count, pos_label='Positive')\n",
        "f1_count = f1_score(y_test_count, y_pred_count, pos_label='Positive')\n",
        "\n",
        "# Generate classification report\n",
        "from sklearn.metrics import classification_report  # Import if needed\n",
        "report_count = classification_report(y_test_count, y_pred_count)\n",
        "\n",
        "# Create DataFrame for CountVectorizer evaluation\n",
        "count_eval_df = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
        "    'Score': [accuracy_count, precision_count, recall_count, f1_count]\n",
        "})\n",
        "\n",
        "print(\"CountVectorizer Evaluation:\")\n",
        "print(count_eval_df)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c916a61c-6f0f-43bc-b7b2-ea5061ace82f",
        "id": "mlnCBJu0gRVM"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer Evaluation:\n",
            "      Metric     Score\n",
            "0   Accuracy  0.500000\n",
            "1  Precision  0.500000\n",
            "2     Recall  1.000000\n",
            "3   F1-Score  0.666667\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00         1\n",
            "    Positive       0.50      1.00      0.67         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhanced Model"
      ],
      "metadata": {
        "id": "xOb7n2n7TjcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from spacy.lang.en import English\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Download 'wordnet' data before using WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatize the tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return ' '.join(tokens)  # Join tokens back into a string\n",
        "\n",
        "# Create a pipeline with preprocessing\n",
        "model = make_pipeline(\n",
        "    TfidfVectorizer(preprocessor=preprocess_text),\n",
        "    MultinomialNB()\n",
        ")\n",
        "model.fit(texts, labels)\n",
        "\n",
        "# Accept user input\n",
        "user_input = input(\"Enter a sentence for sentiment analysis: \")\n",
        "\n",
        "# Preprocess user input before prediction\n",
        "processed_input = preprocess_text(user_input)\n",
        "\n",
        "# Predict the sentiment\n",
        "prediction = model.predict([processed_input])\n",
        "print(\"Predicted sentiment: \", prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCTai3VUTTGJ",
        "outputId": "dbf6ca6d-832f-49c6-8a87-67300952788c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence for sentiment analysis: the movie was fantastic\n",
            "Predicted sentiment:  ['Positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming 'texts' and 'labels' are already defined in your environment\n",
        "# and you have a 'preprocess_text' function (if needed)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\")  # No need to import\n",
        "x = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Assign labels (assuming 'labels' is already defined)\n",
        "y = labels\n",
        "\n",
        "# Split into training and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)  # No need to import\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=1000)  # No need to import\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, pos_label='Positive')\n",
        "recall = recall_score(y_test, y_pred, pos_label='Positive')\n",
        "f1 = f1_score(y_test, y_pred, pos_label='Positive')\n",
        "\n",
        "# Generate classification report\n",
        "from sklearn.metrics import classification_report  # Import if needed\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Create a DataFrame for evaluation metrics\n",
        "eval_df = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
        "    'Score': [accuracy, precision, recall, f1]\n",
        "})\n",
        "print(\"TFIDVectorizer Evaluation:\")\n",
        "print(eval_df)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ0hCdkTW_py",
        "outputId": "c5c20c3f-5fe4-41a3-cb6e-0d226c094778"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFIDVectorizer Evaluation:\n",
            "      Metric  Score\n",
            "0   Accuracy    0.5\n",
            "1  Precision    0.0\n",
            "2     Recall    0.0\n",
            "3   F1-Score    0.0\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.50      1.00      0.67         1\n",
            "    Positive       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion of Results\n",
        "\n",
        "The basic CountVectorizer with MultinomialNB provided a simple baseline, while CountVectorizer with Logistic Regression offered the potential to handle more complex relationships. The enhanced model, incorporating lemmatization and TF-IDF, further improved feature extraction.\n",
        "\n",
        "While combining TF-IDF with Logistic Regression often yields robust sentiment predictions, in this specific scenario, it resulted in lower evaluation scores compared to CountVectorizer. This observation is likely attributed to the limited data size and TF-IDF's inherent focus on distinctive words, which might not be optimal for smaller datasets. Each approach, however, presented trade-offs, such as the simplicity of CountVectorizer with MultinomialNB versus the potential for overfitting with Logistic Regression.\n",
        "\n",
        "Model evaluation involved analyzing classification reports and metrics like accuracy, precision, recall, and F1-score. By comparing these metrics across different models, we could identify the best-performing approach for the given task. The classification reports provided detailed insights into the model's performance for each sentiment class. Overall, this analysis showcased the trade-offs and considerations involved in selecting appropriate techniques for sentiment analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "-QfziT2mhPF0"
      }
    }
  ]
}